# docker-compose.yaml — Centrifuge (API + 2 workers + Postgres + MinIO + LiteLLM proxy)
# Notes:
# - Workers form a scalable pool. Add more worker services or use replicas  # ──────────────────────────────────────────────────────────────────────────────
  # LiteLLM proxy (OpenAI-compatible gateway) - COMMENTED OUT FOR POC SIMPLIFICATION
  #
  # For production deployments, consider using LiteLLM proxy for:
  # - Centralized API key management
  # - Request/response logging and monitoring
  # - Rate limiting and cost tracking
  # - Model routing and load balancing
  # - Multiple provider support (OpenAI, Anthropic, etc.)
  #
  # To re-enable proxy mode:
  # 1. Uncomment this service
  # 2. Add litellm dependencies back to other services
  # 3. Replace OPENAI_API_KEY with LLM_BASE_URL in environment
  # 4. Update core/llm_client.py to use HTTP calls instead of direct library
  #
  # litellm:
  #   image: ghcr.io/berriai/litellm:latest
  #   container_name: centrifuge-litellm
  #   env_file:
  #     - .env
  #   environment:
  #     # OPENAI_API_KEY will be loaded from .env file via env_file directive
  #     LITELLM_MODEL: "openai/gpt-5"
  #     LITELLM_LOG: "WARN"
  #   command: ["litellm", "--port", "4000"]
  #   ports:
  #     - "4000:4000"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:4000/health/livenessProbe"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 30
# - Postgres is the source of truth (runs, recipes, schemas, artifacts index, mapping cache).
# - MinIO stores artifacts (cleaned.csv, errors.csv, diff.csv, audit.ndjson, manifest.json, metrics.json).
# - LiteLLM runs as an OpenAI-compatible proxy gateway for centralized routing/keys/logs.
# - Kafka/Redpanda is intentionally commented out (future production event bus).
# - Redis is omitted for the PoC; the canonical mapping cache lives in Postgres.
services:
  # ──────────────────────────────────────────────────────────────────────────────
  # HTTP API (stateless). Creates runs, exposes status/artifacts. No long work.
  # Same codebase as workers; different entrypoint.
  centrifuge-app:
    image: python:3.13-slim
    container_name: centrifuge-app
    working_dir: /srv/centrifuge
    command: >
      sh -c "pip install -r requirements.txt &&
             uvicorn api.main:app --host 0.0.0.0 --port 8080 --workers 2"
    ports:
      - "8080:8080"
    env_file:
      - .env
    environment:
      # Core infra
      DATABASE_URL: "postgresql://postgres:postgres@postgres:5432/centrifuge"
      ARTIFACT_ENDPOINT: "http://minio:9000"
      ARTIFACT_BUCKET: "artifacts"
      ARTIFACT_ACCESS_KEY: "minioadmin"
      ARTIFACT_SECRET_KEY: "minioadmin"
      ARTIFACT_REGION: "us-east-1"
      # LLM configuration (OPENAI_API_KEY loaded from .env)
      LLM_MODEL_ID: "openai/gpt-5" # gpt-5 for PoC for "general purpose" but highly dependent on the type of data sets
      LLM_TEMPERATURE: "0"
      LLM_SEED: "42"
      # Orchestrator defaults (read by app for validation and by workers during runs)
      EDIT_CAP_PCT: "20"
      CONFIDENCE_FLOOR: "0.80"
      PYTHONUNBUFFERED: "1"
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ./:/srv/centrifuge:rw

  # ──────────────────────────────────────────────────────────────────────────────
  # Workers: each claims one run and executes end-to-end (ingest → validate → rules → LLM → artifacts).
  # Start with 2 to demonstrate concurrency; scale to N by adding more worker services.
  centrifuge-worker-1:
    image: python:3.13-slim
    container_name: centrifuge-worker-1
    working_dir: /srv/centrifuge
    command: >
      sh -c "pip install -r requirements.txt &&
             python worker/main.py"
    env_file:
      - .env
    environment:
      DATABASE_URL: "postgresql://postgres:postgres@postgres:5432/centrifuge"
      ARTIFACT_ENDPOINT: "http://minio:9000"
      ARTIFACT_BUCKET: "artifacts"
      ARTIFACT_ACCESS_KEY: "minioadmin"
      ARTIFACT_SECRET_KEY: "minioadmin"
      ARTIFACT_REGION: "us-east-1"
      # LLM configuration (OPENAI_API_KEY loaded from .env)
      LLM_MODEL_ID: "openai/gpt-5"
      LLM_TEMPERATURE: "0"
      LLM_SEED: "42"
      EDIT_CAP_PCT: "20"
      CONFIDENCE_FLOOR: "0.80"
      WORKER_ID: "worker-1"
      PYTHONUNBUFFERED: "1"
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ./:/srv/centrifuge:rw

  centrifuge-worker-2:
    image: python:3.13-slim
    container_name: centrifuge-worker-2
    working_dir: /srv/centrifuge
    command: >
      sh -c "pip install -r requirements.txt &&
             python worker/main.py"
    env_file:
      - .env
    environment:
      DATABASE_URL: "postgresql://postgres:postgres@postgres:5432/centrifuge"
      ARTIFACT_ENDPOINT: "http://minio:9000"
      ARTIFACT_BUCKET: "artifacts"
      ARTIFACT_ACCESS_KEY: "minioadmin"
      ARTIFACT_SECRET_KEY: "minioadmin"
      ARTIFACT_REGION: "us-east-1"
      # LLM configuration (OPENAI_API_KEY loaded from .env)
      LLM_MODEL_ID: "openai/gpt-5"
      LLM_TEMPERATURE: "0"
      LLM_SEED: "42"
      EDIT_CAP_PCT: "20"
      CONFIDENCE_FLOOR: "0.80"
      WORKER_ID: "worker-2"
      PYTHONUNBUFFERED: "1"
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ./:/srv/centrifuge:rw

  # ──────────────────────────────────────────────────────────────────────────────
  # Postgres: source of truth for runs, recipes, schemas, artifacts index, mapping cache.
  postgres:
    image: ankane/pgvector:latest
    container_name: centrifuge-postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: centrifuge
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d centrifuge"]
      interval: 5s
      timeout: 3s
      retries: 20
    volumes:
      - pg_data:/var/lib/postgresql/data
      - ./ops/sql/init:/docker-entrypoint-initdb.d:ro

  # MinIO: S3-compatible storage for artifacts.
  minio:
    image: minio/minio:latest
    container_name: centrifuge-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # Web console
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 30
    volumes:
      - minio_data:/data

  # One-shot MinIO setup to create 'artifacts' bucket.
  minio-setup:
    image: minio/mc:latest
    container_name: centrifuge-minio-setup
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
        mc alias set local http://minio:9000 minioadmin minioadmin &&
        mc mb -p local/artifacts || true &&
        mc policy set download local/artifacts || true
      "

  # ──────────────────────────────────────────────────────────────────────────────
  # Future production event bus (commented out for PoC). Keep for reviewer context.
  # redpanda:
  #   image: redpandadata/redpanda:latest
  #   container_name: centrifuge-redpanda
  #   command:
  #     - redpanda start
  #     - --overprovisioned
  #     - --smp=1
  #     - --memory=1G
  #     - --reserve-memory=0M
  #     - --node-id=0
  #     - --kafka-addr=PLAINTEXT://0.0.0.0:9092
  #     - --advertise-kafka-addr=PLAINTEXT://redpanda:9092
  #     - --pandaproxy-addr=0.0.0.0:8082
  #     - --advertise-pandaproxy-addr=redpanda:8082
  #   ports:
  #     - "9092:9092"
  #     - "8082:8082"
  #   volumes:
  #     - redpanda_data:/var/lib/redpanda/data
  #
  # redpanda-console:
  #   image: redpandadata/console:latest
  #   container_name: centrifuge-redpanda-console
  #   environment:
  #     KAFKA_BROKERS: redpanda:9092
  #   ports:
  #     - "8083:8080"
  #   depends_on:
  #     - redpanda

volumes:
  pg_data:
  minio_data:
  # redpanda_data:
